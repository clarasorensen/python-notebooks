{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercises** (in bold) involve running the code provided, examining the outputs, and answering the questions on the handout, which you will submit after you're done. These count towards your reading responses+in-class exercise grade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setup:\n",
    "import numpy as np\n",
    "import urllib2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def load(url):\n",
    "    \"\"\"read a CSV from the web, return data and labels\"\"\"\n",
    "    response = urllib2.urlopen(url)\n",
    "    Xy = np.loadtxt(response, delimiter=',')\n",
    "    y = Xy[:, -1]\n",
    "    X = Xy[:, :-1]\n",
    "    return X, y\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def knn_predscore(trainX, trainy, testX, testy, k, metric='euclidean'):\n",
    "    model = KNeighborsClassifier(n_neighbors=k,\n",
    "                                metric=metric)\n",
    "\n",
    "    model.fit(trainX, trainy)\n",
    "    return model.score(testX, testy)\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "def perceptron_predscore(trainX, trainy, testX, testy):\n",
    "    model = Perceptron(n_iter=200)\n",
    "    model.fit(trainX, trainy)\n",
    "    return model.score(testX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST images as 784-dimensional vectors (each dim a pixel) as in PS1. Keep only 2 digits for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainXraw, trainyraw = load('http://cs.wellesley.edu/~sravana/ml/ps1/data/mnist1100/training.txt')\n",
    "testXraw, testyraw = load('http://cs.wellesley.edu/~sravana/ml/ps1/data/mnist1100/testing.txt')\n",
    "print 'Loaded training data', trainXraw.shape, trainyraw.shape, 'and testing data', testXraw.shape, testyraw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_2digits(c1, c2, trainXraw, trainyraw, testXraw, testyraw):\n",
    "    \"\"\"filter the full MNIST dataset to two digits for binary classification\"\"\"\n",
    "    trainX = trainXraw[(trainyraw == c1) | (trainyraw == c2), :]\n",
    "    trainy = trainyraw[(trainyraw == c1) | (trainyraw == c2)]\n",
    "\n",
    "    testX = testXraw[(testyraw == c1) | (testyraw == c2), :]\n",
    "    testy = testyraw[(testyraw == c1) | (testyraw == c2)]\n",
    "    \n",
    "    labelmap = {c1: -1, c2: +1}  # arbitrary assignment of +1 and -1 to the two classes\n",
    "    for i, label in enumerate(testy):\n",
    "        testy[i] = labelmap[label]\n",
    "    for i, label in enumerate(trainy):\n",
    "        trainy[i] = labelmap[label]\n",
    "    \n",
    "    print 'Loaded', trainy.size, 'training points and', testy.size, 'testing points for digits', c1, 'and', c2\n",
    "    return trainX, trainy, testX, testy, labelmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider two digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX, trainy, testX, testy, labelmap = load_2digits(8, 9, trainXraw, trainyraw, testXraw, testyraw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the mean images and w.\n",
    "\n",
    "Let's visualize the \"average\" image in our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "meanimg = np.mean(trainX, axis=0)  # get mean of each feature\n",
    "# visualize mean image for fun\n",
    "plt.imshow(meanimg.reshape(28, 28), interpolation='none', cmap = plt.get_cmap('gray'), vmin = 0, vmax = 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following two cells visualize the average of *each* digit separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c1 = labelmap.keys()[0]\n",
    "print 'Visualizing', c1, '(', labelmap[c1], ')'\n",
    "meanc1 = np.mean(trainX[trainy==labelmap[c1], :], axis=0)\n",
    "plt.imshow(meanc1.reshape(28, 28), interpolation='none', cmap = plt.get_cmap('gray'), vmin = 0, vmax = 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c2 = labelmap.keys()[1]\n",
    "print 'Visualizing', c2, '(', labelmap[c2], ')'\n",
    "meanc2 = np.mean(trainX[trainy==labelmap[c2], :], axis=0)\n",
    "plt.imshow(meanc2.reshape(28, 28), interpolation='none', cmap = plt.get_cmap('gray'), vmin = 0, vmax = 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight vector learned by the perceptron to distinguish the 2 digits is itself a 784-dim vector, since the data lives in 784 dimensions. Just for fun, let us visualize it as an image!\n",
    "\n",
    "**Exercise a**: According to the colorbar legend, green represents 0, red represents positive values, and blue negative. Why is the weight vector green around the boundaries?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "model = Perceptron(n_iter=200, fit_intercept=False)   # no bias fitting\n",
    "model.fit(trainX, trainy)\n",
    "w = model.coef_[0]\n",
    "w/=norm(w, 2)  # normalize\n",
    "\n",
    "plt.imshow(w.reshape(28, 28), cmap = plt.get_cmap('jet'), \n",
    "           vmin = np.min(w), vmax = np.max(w))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Baseline\n",
    "\n",
    "**Exercise b:** Write down the resulting accuracies in the worksheet for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trainX, trainy, testX, testy, labelmap = load_2digits(1, 9, trainXraw, trainyraw, testXraw, testyraw)\n",
    "print 'kNN accuracy on original feature rep is', knn_predscore(trainX, trainy, testX, testy, 3)\n",
    "print 'Perceptron accuracy on original feature rep is', perceptron_predscore(trainX, trainy, testX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Center and Variance-Scale the Features\n",
    "\n",
    "### 1a: Centering\n",
    "\n",
    "Compute the mean for each feature in the vector space.\n",
    "Center the points by subtracting the mean.\n",
    "\n",
    "$$f_{i, j}-\\mu_j$$\n",
    "\n",
    "Let's see what happens for a toy example in 2-d space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scatterplot_labeledpoints(X, y):\n",
    "    \"\"\"plot 2-d points, colored with one of two labels (-1 or 1)\"\"\"\n",
    "    plt.scatter(X[y==1, 0], X[y==1, 1], color='b')\n",
    "    plt.scatter(X[y==-1, 0], X[y==-1, 1], color='r')\n",
    "    plt.axis([min(0, min(X[:, 0])), max(X[:, 0])*1.1, min(0, min(X[:, 1])), max(X[:, 1])*1.1])\n",
    "    plt.show()\n",
    "    \n",
    "# original points\n",
    "toyX = np.random.rand(100, 2)   # 100 points in 2d\n",
    "toyy = np.ones(100)\n",
    "toyy[50:] = -1\n",
    "scatterplot_labeledpoints(toyX, toyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# after centering\n",
    "meantoy = np.mean(toyX, axis=0)\n",
    "centeredToyX = toyX - meantoy\n",
    "scatterplot_labeledpoints(centeredToyX, toyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise c:** from the plots, how did the toy data change after centering?\n",
    "\n",
    "Now let's see how centering affects classification accuracy on the MNIST digits. Remember that we're working with a binary dataset of two digits, so we can apply the perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "centeredTrainX = trainX-meanimg\n",
    "centeredTestX = testX-meanimg # can't use means of test data (or it would be cheating); must use training means\n",
    "    \n",
    "print 'kNN accuracy on centered feature rep is', knn_predscore(centeredTrainX, trainy, centeredTestX, testy, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise d:** Explain why the kNN accuracy didn't change.\n",
    "\n",
    "### 1b: Variance Normalization\n",
    "\n",
    "Compute the standard deviation for each of the features and normalize each vector.\n",
    "\n",
    "$$\\dfrac{f_{i, j}}{\\sigma_j}$$\n",
    "\n",
    "Here's a toy data set where the two features are on different scales. One is between 50 and 75 and the other is between 0 and 100.\n",
    "\n",
    "**Exercise e:** While this is a synthetic (randomly generated) dataset, you can imagine this happening in naturally occurring featurized data as well. If each datapoint represents a person, name 2 numerical features about a person (physical attributes, etc) that would be on different scales (50 to 75, 0 to 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toyXvar = np.random.rand(100, 2)   # 100 points in 2d\n",
    "toyXvar[:, 0]*=100\n",
    "toyXvar[:, 1]*=25\n",
    "toyXvar[:, 1]+=50\n",
    "\n",
    "toyy = np.ones(100)\n",
    "toyy[50:] = -1\n",
    "scatterplot_labeledpoints(toyXvar, toyy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# after scaling and centering\n",
    "stds = np.std(toyXvar, axis=0)\n",
    "stdnormtoyXvar = (toyXvar-np.mean(toyXvar, axis=0))/stds\n",
    "scatterplot_labeledpoints(stdnormtoyXvar, toyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise f:** How did the shape of the data change after centering and variance scaling? Observe the scales on the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Example-Norm the Data-Points\n",
    "\n",
    "**Note: this experiment will help you answer PS2 Q7, so observe it closely.**\n",
    "\n",
    "Scale every data-point (row) by its Euclidean norm.\n",
    "\n",
    "$$\\dfrac{x}{||x||}$$\n",
    "\n",
    "Here's the example from the slides of news articles in terms of number of mentions of \"Wellesley\" and \"students\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toyX2 = np.array([[3, 10], [13, 34], [6, 1]])\n",
    "toyy2 = np.array([1, 1, -1])\n",
    "print toyX2\n",
    "scatterplot_labeledpoints(toyX2, toyy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we learned a hyperplane parameterized by `w=[-3, 1], b = 1`. Let's compute the distance of each of these point to the hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w = np.array([-3, 1])\n",
    "b = 1\n",
    "for i, point in enumerate(toyX2):\n",
    "    print point, 'with label', toyy2[i], 'lies at distance', w.dot(point) + b, 'from hyperplane'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now after normalizing each example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "toyX2norms = norm(toyX2, axis=1, keepdims=True)\n",
    "exnormToyX2 = toyX2/toyX2norms\n",
    "print exnormToyX2\n",
    "scatterplot_labeledpoints(exnormToyX2, toyy2)\n",
    "\n",
    "for point in exnormToyX2:\n",
    "    print w.dot(point) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, point in enumerate(exnormToyX2):\n",
    "    print point, 'with label', toyy2[i], 'lies at distance', w.dot(point) + 1, 'from hyperplane'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What changed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Simpler Feature Design\n",
    "\n",
    "Using every pixel's real value as feature may be redundant, and comes at the cost of slowing down our programs. Can we get away with simpler features?\n",
    "\n",
    "### Binarize the Feature Values\n",
    "\n",
    "Make every feature value in the MNIST data 0 or 1 by thresholding (remove grays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binTrainX = np.zeros(trainX.shape)\n",
    "binTrainX[trainX>50] = 1\n",
    "binTestX = np.zeros(testX.shape)\n",
    "binTestX[testX>50] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'kNN accuracy on binarized feature rep is', knn_predscore(binTrainX, trainy, binTestX, testy, 3)\n",
    "print 'Perceptron accuracy on binarized feature rep is', perceptron_predscore(binTrainX, trainy, binTestX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Based Feature Representations\n",
    "\n",
    "Here's one simple way of featurizing the image using a small number of dimensions: represent the data points as R-dimensional vectors consisting of the number of black pixels in each of the R regions of the image. \n",
    "\n",
    "We divide up the image regions by flattening it into a 784-dimensional vector row-wise. Each block of 784/R pixels is a region.\n",
    "\n",
    "Does this give us enough information for classification?\n",
    "\n",
    "**Exercise g:** Try out a few different values of R to see how the accuracy changes. Can you beat the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def numblack(X, regions):\n",
    "    Xregions = []\n",
    "    dims = X.shape[1]\n",
    "    for i in range(regions):\n",
    "        if i==0:\n",
    "            tmp = X[:, :dims/regions]\n",
    "        else:\n",
    "            tmp = X[:, dims*(i-1)/regions:dims*i/regions]\n",
    "        Xregions.append(np.sum(tmp==0, axis=1))\n",
    "    return np.vstack(tuple(Xregions)).T\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = 2\n",
    "blackTestX = numblack(testX, R)\n",
    "blackTrainX = numblack(trainX, R)\n",
    "print 'kNN accuracy on ', R, 'dim feature rep is', knn_predscore(blackTrainX, trainy, blackTestX, testy, 3, 'euclidean')\n",
    "print 'Perceptron accuracy on', R, 'dim feature rep is', perceptron_predscore(blackTrainX, trainy, blackTestX, testy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise h:** Why might we want to featurize an image by regions rather than pixels? (There are two possible motives, name either one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
